{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Needed Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch matplotlib numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing packages for optim\n",
    "# Muon\n",
    "!git clone https://github.com/KellerJordan/Muon.git\n",
    "!pip install -e Muon\n",
    "# SOAP\n",
    "!git clone https://github.com/nikhilvyas/SOAP.git\n",
    "# Shampoo\n",
    "!git config core.sparseCheckout true\n",
    "!git clone --filter=blob:none --no-checkout https://github.com/google-research/google-research/  google_research\n",
    "%cd google_research/\n",
    "!git sparse-checkout set --cone\n",
    "!git checkout master\n",
    "!git sparse-checkout set scalable_shampoo\n",
    "!pip install -r scalable_shampoo/requirements.txt\n",
    "%cd ..\n",
    "\n",
    "# Pleas check the repo before\n",
    "import os, sys\n",
    "sys.path.append(os.path.join(os.getcwd(),\"google_research/scalable_shampoo/pytorch\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Imported Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "from google_research.scalable_shampoo.pytorch.shampoo import Shampoo\n",
    "from muon import Muon\n",
    "from SOAP.soap import SOAP\n",
    "\n",
    "from typing import Dict, Callable,Union\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Handle distributed optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for distributed Muon (this is on one GPU)\n",
    "import torch.distributed as dist\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "backend = 'nccl' if torch.cuda.is_available() else 'gloo'\n",
    "\n",
    "\n",
    "# Set necessary environment variables\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "os.environ[\"MASTER_ADDR\"] = \"127.0.0.1\"\n",
    "os.environ[\"MASTER_PORT\"] = \"29500\"\n",
    "\n",
    "# Initialize the process group\n",
    "if not dist.is_initialized():\n",
    "    dist.init_process_group(backend=backend, init_method=\"env://\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  The Rosenbrock Function and Its Non-Convexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction\n",
    "The **Rosenbrock function**, also known as the **banana function**, is a common test problem for optimization algorithms. It is widely used to assess the performance of numerical optimization methods due to its challenging non-convex landscape.\n",
    "\n",
    "## Definition\n",
    "The Rosenbrock function in two dimensions is defined as:\n",
    "\n",
    "$$ f(x, y) = (a - x)^2 + b(y - x^2)^2 $$\n",
    "\n",
    "where typically **a = 1** and **b = 100**.\n",
    "\n",
    "The function can be generalized to higher dimensions as (we take n=2):\n",
    "\n",
    "$$ f(\\mathbf{x}) = \\sum_{i=1}^{n-1} \\, [a - x_i]^2 + b(x_{i+1} - x_i^2)^2 $$\n",
    "\n",
    "## Properties\n",
    "- **Global Minimum:** At \\( x^* = (1,1) \\), where \\( f(x^*) = 0 \\).\n",
    "- **Non-Convexity:** The function has a narrow, curved valley leading to the minimum, making gradient-based optimization difficult.\n",
    "- **Plateaus & Steep Slopes:** Optimization algorithms struggle due to flat regions and steep inclines.\n",
    "\n",
    "\n",
    "## Optimization Challenge\n",
    "- The **narrow valley** leads to slow convergence in gradient-based methods like **gradient descent**.\n",
    "- **Newtonâ€™s method** may perform better but requires second-order derivatives.\n",
    "- **Evolutionary algorithms** (e.g., Genetic Algorithms, CMA-ES) can be more effective in escaping local traps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rosenbrock(x: float, y: float, a: float = 1, b: float = 100) -> float:\n",
    "    \"\"\"\n",
    "    Computes the Rosenbrock function value for given x and y.\n",
    "    \n",
    "    Parameters:\n",
    "        x (float): The x-coordinate.\n",
    "        y (float): The y-coordinate.\n",
    "        a (float, optional): Constant parameter, default is 1.\n",
    "        b (float, optional): Constant parameter, default is 100.\n",
    "    \n",
    "    Returns:\n",
    "        float: The computed Rosenbrock function value.\n",
    "    \"\"\"\n",
    "    return (a - x) ** 2 + b * (y - x**2) ** 2\n",
    "\n",
    "def torch_rosenbrock(params: Union[torch.Tensor, list], a: float = 1, b: float = 100) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the Rosenbrock function value for given parameters in a PyTorch-compatible form.\n",
    "    \n",
    "    Parameters:\n",
    "        params (Union[torch.Tensor, list]): A tensor or list containing x and y values.\n",
    "        a (float, optional): Constant parameter, default is 1.\n",
    "        b (float, optional): Constant parameter, default is 100.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: The computed Rosenbrock function value as a PyTorch tensor.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        x, y = params\n",
    "        return (a - x) ** 2 + b * (y - x**2) ** 2\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Invalid input for torch_rosenbrock: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Visualization in 2D "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def rosenbrock(x, y, a=1, b=100):\n",
    "    return (a - x)**2 + b * (y - x**2)**2\n",
    "\n",
    "x = np.linspace(-2, 2, 400)\n",
    "y = np.linspace(-1, 3, 400)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = rosenbrock(X, Y)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.contourf(X, Y, Z, levels=50, cmap='plasma',vmin=-1)\n",
    "plt.colorbar(label='Function Value')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Contour Plot of the Rosenbrock Function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import Ademamix code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Code from repo https://github.com/nanowell/AdEMAMix-Optimizer-Pytorch/blob/main/AdEMAMix.py\n",
    "\n",
    "class AdEMAMix(optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999, 0.9999), eps=1e-8,\n",
    "                 weight_decay=0, alpha=5.0, T_alpha_beta3=None):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(f\"Invalid epsilon value: {eps}\")\n",
    "        assert len(betas) == 3, f\"Invalid beta parameters: {betas}, expected 3\"\n",
    "        assert all(0.0 <= beta < 1.0 for beta in betas), f\"Invalid beta parameters: {betas}\"\n",
    "        if not 0.0 <= weight_decay:\n",
    "            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n",
    "        \n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay,\n",
    "                        alpha=alpha, T_alpha_beta3=T_alpha_beta3)\n",
    "        super(AdEMAMix, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(AdEMAMix, self).__setstate__(state)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            params_with_grad = []\n",
    "            grads = []\n",
    "            exp_avgs = []\n",
    "            exp_avg_sqs = []\n",
    "            exp_avg_slow = []\n",
    "            state_steps = []\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is not None:\n",
    "                    params_with_grad.append(p)\n",
    "                    if p.grad.is_sparse:\n",
    "                        raise RuntimeError('AdEMAMix does not support sparse gradients')\n",
    "                    grads.append(p.grad)\n",
    "\n",
    "                    state = self.state[p]\n",
    "                    # Lazy state initialization\n",
    "                    if len(state) == 0:\n",
    "                        state['step'] = 0\n",
    "                        # Exponential moving average of gradient values\n",
    "                        state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                        # Exponential moving average of squared gradient values\n",
    "                        state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                        # Slow exponential moving average\n",
    "                        state['exp_avg_slow'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "\n",
    "                    exp_avgs.append(state['exp_avg'])\n",
    "                    exp_avg_sqs.append(state['exp_avg_sq'])\n",
    "                    exp_avg_slow.append(state['exp_avg_slow'])\n",
    "                    state['step'] += 1\n",
    "                    state_steps.append(state['step'])\n",
    "\n",
    "            beta1, beta2, beta3 = group['betas']\n",
    "            alpha = group['alpha']\n",
    "            T_alpha_beta3 = group['T_alpha_beta3']\n",
    "\n",
    "            self._update_adamemix(\n",
    "                params_with_grad,\n",
    "                grads,\n",
    "                exp_avgs,\n",
    "                exp_avg_sqs,\n",
    "                exp_avg_slow,\n",
    "                state_steps,\n",
    "                beta1=beta1,\n",
    "                beta2=beta2,\n",
    "                beta3=beta3,\n",
    "                alpha=alpha,\n",
    "                T_alpha_beta3=T_alpha_beta3,\n",
    "                lr=group['lr'],\n",
    "                weight_decay=group['weight_decay'],\n",
    "                eps=group['eps'],\n",
    "            )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _update_adamemix(self, params, grads, exp_avgs, exp_avg_sqs, exp_avg_slow, state_steps,\n",
    "                         beta1, beta2, beta3, alpha, T_alpha_beta3, lr, weight_decay, eps):\n",
    "        \n",
    "        for i, param in enumerate(params):\n",
    "            grad = grads[i]\n",
    "            exp_avg = exp_avgs[i]\n",
    "            exp_avg_sq = exp_avg_sqs[i]\n",
    "            exp_avg_slow_i = exp_avg_slow[i]\n",
    "            step = state_steps[i]\n",
    "\n",
    "            bias_correction1 = 1 - beta1 ** step\n",
    "            bias_correction2 = 1 - beta2 ** step\n",
    "\n",
    "            if T_alpha_beta3 is not None:\n",
    "                alpha_t = min(step * alpha / T_alpha_beta3, alpha)\n",
    "                beta3_t = min(math.exp(math.log(beta1) * math.log(beta3) / \n",
    "                              ((1 - step / T_alpha_beta3) * math.log(beta3) + \n",
    "                               (step / T_alpha_beta3) * math.log(beta1))), beta3)\n",
    "            else:\n",
    "                alpha_t = alpha\n",
    "                beta3_t = beta3\n",
    "\n",
    "            # Decay the first and second moment running average coefficient\n",
    "            exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "            exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
    "            exp_avg_slow_i.mul_(beta3_t).add_(grad, alpha=1 - beta3_t)\n",
    "\n",
    "            denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(eps)\n",
    "\n",
    "            step_size = lr / bias_correction1\n",
    "\n",
    "            if weight_decay != 0:\n",
    "                param.add_(param, alpha=-weight_decay * lr)\n",
    "\n",
    "            param.addcdiv_(exp_avg + alpha_t * exp_avg_slow_i, denom, value=-step_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run Optimizers on Rosenbrock problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure setup\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Parameters\n",
    "epochs = 500\n",
    "\n",
    "# Initial condition\n",
    "initial_position = torch.tensor([-1.0, 1.0], dtype=torch.float32, requires_grad=True).to(device)\n",
    "\n",
    "# Path colors\n",
    "colors = {\n",
    "    \"AdamW\": \"yellow\", \"Shampoo\": \"green\", \"SOAP\": \"aqua\", \"Muon\": \"black\", \"Adagrad\": \"red\",\n",
    "    \"SGD\": \"orange\", \"SGDM\": \"blue\", \"AdEMAMix\": \"violet\", \"NAG\": \"grey\", \"RMSprop\": \"brown\"\n",
    "}\n",
    "\n",
    "# Define optimizer parameters\n",
    "try:\n",
    "    optim_params = {\n",
    "        optimizer: initial_position.clone().detach().requires_grad_(True)\n",
    "        if optimizer != \"Muon\"\n",
    "        else initial_position.clone().detach().view(-1, 1).requires_grad_(True)\n",
    "        for optimizer in colors.keys()\n",
    "    }\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Error initializing optimizer parameters: {e}\")\n",
    "\n",
    "# Define optimizers\n",
    "try:\n",
    "    optimizers  = {\n",
    "        \"Shampoo\": Shampoo([optim_params[\"Shampoo\"].to(device)], lr=0.001),\n",
    "        \"AdamW\": optim.AdamW([optim_params[\"AdamW\"].to(device)], lr=0.01, weight_decay=0.01),\n",
    "        \"SOAP\": SOAP([optim_params[\"SOAP\"].to(device)], lr=0.01),\n",
    "        \"Muon\": Muon([optim_params[\"Muon\"].to(device)], lr=0.01, weight_decay=0.01),\n",
    "        \"AdEMAMix\": AdEMAMix([optim_params[\"AdEMAMix\"]], lr=0.01, weight_decay=0.01),\n",
    "        \"RMSprop\": optim.RMSprop([optim_params[\"RMSprop\"]], lr=0.01, weight_decay=0.01),\n",
    "        \"Adagrad\": optim.Adagrad([optim_params[\"Adagrad\"]], lr=0.01, weight_decay=0.01),\n",
    "        \"SGD\": optim.SGD([optim_params[\"SGD\"]], lr=0.001, weight_decay=0.01),\n",
    "        \"SGDM\": optim.SGD([optim_params[\"SGDM\"]], lr=0.001, momentum=0.9, weight_decay=0.01),\n",
    "        \"NAG\": optim.SGD([optim_params[\"NAG\"]], lr=0.001, momentum=0.9, weight_decay=0.01, nesterov=True),\n",
    "    }\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Error initializing optimizers: {e}\")\n",
    "\n",
    "# Create meshgrid for contour plot\n",
    "x = np.linspace(-2, 3, 400)\n",
    "y = np.linspace(-1, 3, 400)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = (1 - X) ** 2 + 100 * (Y - X**2) ** 2  # Rosenbrock function\n",
    "\n",
    "# Set the contour\n",
    "contour = ax.contourf(X, Y, Z, levels=np.logspace(0, 5, 20), cmap='coolwarm', alpha=0.6, vmin=-1, vmax=30)\n",
    "cbar = plt.colorbar(contour, ticks=[0, 10, 1e2, 1e3, 1e4])\n",
    "\n",
    "def get_positions(epochs: int, optim_params: Dict[str, torch.Tensor]) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Run optimization and store the positions of parameters for visualization.\n",
    "    \"\"\"\n",
    "    positions = {key: [param.cpu().clone().detach().numpy()] for key, param in optim_params.items()}\n",
    "    \n",
    "    for t in range(epochs):\n",
    "        for key, optimizer in optimizers.items():\n",
    "            try:\n",
    "                optimizer.zero_grad()\n",
    "                loss = torch_rosenbrock(optim_params[key])  # Compute loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                positions[key].append(optim_params[key].cpu().clone().detach().numpy())\n",
    "            except Exception as e:\n",
    "                print(f\"Error during optimization step for {key}: {e}\")\n",
    "    \n",
    "    return {key: np.array(path) for key, path in positions.items()}\n",
    "\n",
    "def update(frame: int):\n",
    "    \"\"\"\n",
    "    Update function for animation.\n",
    "    \"\"\"\n",
    "    positions = get_positions(epochs, optim_params)\n",
    "    ax.clear()\n",
    "    \n",
    "    # Redraw contour\n",
    "    contour = ax.contourf(X, Y, Z, levels=np.logspace(0, 5, 20), cmap='coolwarm', alpha=0.6, vmin=-1, vmax=30)\n",
    "    \n",
    "    for key, path_pos in positions.items():\n",
    "        try:\n",
    "            ax.plot(path_pos[:frame, 0], path_pos[:frame, 1], color=colors[key], label=f\"{key} Path\", lw=2)\n",
    "            ax.scatter(*path_pos[frame], color=colors[key], s=100, zorder=5)\n",
    "            loss_value = torch_rosenbrock(torch.tensor(path_pos[frame])).item()\n",
    "            ax.text(path_pos[frame, 0], path_pos[frame, 1], f\"{loss_value:.2f}\", color=colors[key], fontsize=12, ha='center', va='bottom')\n",
    "        except Exception as e:\n",
    "            print(f\"Error in animation update for {key}: {e}\")\n",
    "    \n",
    "    ax.set_xlim(-2, 3)\n",
    "    ax.set_ylim(-1, 3)\n",
    "    ax.set_title(r\"$f(x, y) = (1 - x)^2 + 100(y - x^2)^2$\" + f\"\\nEpoch {frame}\", fontsize=12)\n",
    "    ax.legend(loc=\"upper left\")\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"y\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Generate animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def animate_and_save_figure(\n",
    "    fig: plt.Figure,\n",
    "    update: Callable[[int], None],\n",
    "    epochs: int,\n",
    "    interval: int = 100,\n",
    "    repeat: bool = False,\n",
    "    title: str = \"animation\",\n",
    "    fps: int = 15\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Creates an animation and saves it as a GIF.\n",
    "    \n",
    "    Parameters:\n",
    "        fig (plt.Figure): The figure to animate.\n",
    "        update (Callable[[int], None]): The update function for animation.\n",
    "        epochs (int): Number of frames (epochs) for the animation.\n",
    "        interval (int, optional): Interval between frames in milliseconds (default: 10ms).\n",
    "        repeat (bool, optional): Whether the animation should loop (default: False).\n",
    "        title (str, optional): Name of the output GIF file (default: \"animation\").\n",
    "        fps (int, optional): Frames per second for the saved GIF (default: 15).\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ani = FuncAnimation(fig, update, frames=epochs, interval=interval, repeat=repeat)\n",
    "        ani.save(f\"{title}.gif\", writer=\"pillow\", fps=fps)\n",
    "        print(f\"Animation saved as {title}.gif\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error in animation process: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animate_and_save_figure(fig,update,20, 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
